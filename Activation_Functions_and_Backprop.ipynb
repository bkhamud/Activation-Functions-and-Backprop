{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activation Functions and Backprop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbHDFze1KEgO"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "np.random.seed(0) #Set for reproducibility"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XksZGOjFvPVS"
      },
      "source": [
        "#We'll first define our activation function\n",
        "def activation_function(name):\n",
        "    \"\"\"\n",
        "    Returns the mathematical expression of a chosen activation function\n",
        "    either sigmoid or relu\n",
        "\n",
        "    Input:\n",
        "      name: str variable, accepts 'relu' and 'sigmoid'\n",
        "    Output:\n",
        "      returns lambda function mathematical expression for activation function\n",
        "    \"\"\"\n",
        "    if(name == 'sigmoid'):\n",
        "        return lambda x: 1/(1+np.exp(-x))\n",
        "    elif(name == 'relu'):\n",
        "        return lambda x: np.maximum(x, 0)\n",
        "\n",
        "#We also define the derivative of the activation function\n",
        "def grad_activation_function(name):\n",
        "    \"\"\"\n",
        "    Returns the mathematical expression of a chosen activation function's derivative\n",
        "    either sigmoid or relu\n",
        "\n",
        "    Input:\n",
        "      name: str variable, accepts 'relu' and 'sigmoid'\n",
        "    Output:\n",
        "      returns lambda function mathematical expression for derivative of the activation function\n",
        "    \"\"\"\n",
        "    if(name == 'sigmoid'):\n",
        "        sigmoid = lambda x: 1/(1 + np.exp(-x))\n",
        "        return lambda x: sigmoid(x) * (1 - sigmoid(x)) \n",
        "    elif(name == 'relu'):\n",
        "        return lambda x: np.heaviside(x, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ta41_Gq5jgM"
      },
      "source": [
        "#Now define functions for the forward training and backprop\n",
        "def feedforward(self, x):\n",
        "   \"\"\"\n",
        "   Helper function to encode the feed forward behavior of a network, a given \n",
        "   layer will go as:\n",
        "   activation_function(weights * input + bias)\n",
        "   WHich is then fed to the next layer.\n",
        "   Input:\n",
        "      x: input vector\n",
        "   Output:\n",
        "      z_s: list of inputs to activation function in a given layer\n",
        "      a_s: activation function outputs\n",
        "   \"\"\"\n",
        "   a = np.copy(x)\n",
        "   z_s = []\n",
        "   a_s = [a]\n",
        "   for i in range(len(self.weights)):\n",
        "      activation_function = self.activation_function(self.activations[i])\n",
        "      z_s.append(self.weights[i].dot(a) + self.biases[i])\n",
        "      a = activation_function(z_s[-1])\n",
        "      a_s.append(a)\n",
        "   return (z_s, a_s)\n",
        "\n",
        "def backpropagation(self, y, z_s, a_s):\n",
        "   \"\"\"\n",
        "   Helper function to encode the backprop of a network, a given \n",
        "   layer will go as:\n",
        "   activation_function(weights * input + bias)\n",
        "   WHich is then fed to the next layer.\n",
        "   Input:\n",
        "      y: true values of the function we wish to approximate\n",
        "      z_s: list of \n",
        "      a_s: list of \n",
        "   Output:\n",
        "      dw: derivative of weight matrix\n",
        "      db: derivative of bias matrix\n",
        "   \"\"\"\n",
        "   dw = []  # dJ/dw\n",
        "   db = []  # dJ/db\n",
        "   deltas = [None] * len(self.weights)  #error for each layer\n",
        "   #Retrieve error from previous layer\n",
        "   deltas[-1] = ((y-a_s[-1])*(self.grad_activation_function(self.activations[-1]))(z_s[-1]))\n",
        "   #Perform BackPropagation\n",
        "   for i in reversed(range(len(deltas)-1)):\n",
        "      deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.grad_activation_function(self.activations[i])(z_s[i]))        \n",
        "   batch_size = y.shape[1]\n",
        "   db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas]\n",
        "   dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]\n",
        "   return dw, db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm5eSLzyBu0B",
        "outputId": "245b525e-a035-4356-95b8-0fcc326ee6b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "class NeuralNetwork(object):\n",
        "    def __init__(self, layers = [2 , 10, 1], activations=['sigmoid', 'sigmoid']):\n",
        "        self.layers = layers\n",
        "        self.activations = activations\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        for i in range(len(layers)-1):\n",
        "            self.weights.append(np.random.randn(layers[i+1], layers[i]))\n",
        "            self.biases.append(np.random.randn(layers[i+1], 1))\n",
        "    \n",
        "    def feedforward(self, x):\n",
        "        \"\"\"\n",
        "        Helper function to encode the feed forward behavior of a network, a given \n",
        "        layer will go as:\n",
        "        activation_function(weights * input + bias)\n",
        "        WHich is then fed to the next layer.\n",
        "        Input:\n",
        "            x: input vector\n",
        "        Output:\n",
        "            z_s: list of inputs to activation function in a given layer\n",
        "            a_s: activation function outputs\n",
        "        \"\"\"\n",
        "        a = np.copy(x)\n",
        "        z_s = []\n",
        "        a_s = [a]\n",
        "        for i in range(len(self.weights)):\n",
        "            activation_function = self.activation_function(self.activations[i])\n",
        "            z_s.append(self.weights[i].dot(a) + self.biases[i])\n",
        "            a = activation_function(z_s[-1])\n",
        "            a_s.append(a)\n",
        "        return (z_s, a_s)\n",
        "\n",
        "    def backpropagation(self, y, z_s, a_s):\n",
        "        \"\"\"\n",
        "        Helper function to encode the backprop of a network, a given \n",
        "        layer will go as:\n",
        "        activation_function(weights * input + bias)\n",
        "        WHich is then fed to the next layer.\n",
        "        Input:\n",
        "            y: true values of the function we wish to approximate\n",
        "            z_s: list of \n",
        "            a_s: list of \n",
        "        Output:\n",
        "            dw: derivative of weight matrix\n",
        "            db: derivative of bias matrix\n",
        "        \"\"\"\n",
        "        dw = []  # dJ/dw\n",
        "        db = []  # dJ/db\n",
        "        deltas = [None] * len(self.weights)  #error for each layer\n",
        "        #Retrieve error from previous layer\n",
        "        deltas[-1] = ((y-a_s[-1])*(self.grad_activation_function(self.activations[-1]))(z_s[-1]))\n",
        "        #Perform BackPropagation\n",
        "        for i in reversed(range(len(deltas)-1)):\n",
        "            deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.grad_activation_function(self.activations[i])(z_s[i]))        \n",
        "        batch_size = y.shape[1]\n",
        "        db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas]\n",
        "        dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]\n",
        "        return dw, db\n",
        "\n",
        "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
        "        for e in range(epochs): \n",
        "            i=0\n",
        "            while(i<len(y)):\n",
        "                x_batch = x[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "                i = i+batch_size\n",
        "                z_s, a_s = self.feedforward(x_batch)\n",
        "                dw, db = self.backpropagation(y_batch, z_s, a_s)\n",
        "                self.weights = [w+lr*dweight for w,dweight in  zip(self.weights, dw)]\n",
        "                self.biases = [w+lr*dbias for w,dbias in  zip(self.biases, db)]\n",
        "                #print(\"loss = {}\".format(np.linalg.norm(a_s[-1]-y_batch)))\n",
        "\n",
        "    @staticmethod\n",
        "    def activation_function(name):\n",
        "        \"\"\"\n",
        "        Returns the mathematical expression of a chosen activation function\n",
        "        either sigmoid or relu\n",
        "\n",
        "        Input:\n",
        "          name: str variable, accepts 'relu' and 'sigmoid'\n",
        "        Output:\n",
        "          returns lambda function mathematical expression for activation function\n",
        "        \"\"\"\n",
        "        if(name == 'sigmoid'):\n",
        "            return lambda x: 1/(1 + np.exp(-x))\n",
        "        elif(name == 'relu'):\n",
        "            return lambda x: np.maximum(x, 0)\n",
        "    \n",
        "    @staticmethod\n",
        "    def grad_activation_function(name):\n",
        "        \"\"\"\n",
        "        Returns the mathematical expression of a chosen activation function's derivative\n",
        "        either sigmoid or relu\n",
        "\n",
        "        Input:\n",
        "          name: str variable, accepts 'relu' and 'sigmoid'\n",
        "        Output:\n",
        "          returns lambda function mathematical expression for derivative of the activation function\n",
        "        \"\"\"\n",
        "        if(name == 'sigmoid'):\n",
        "            sigmoid = lambda x: 1/(1 + np.exp(-x))\n",
        "            return lambda x: sigmoid(x) * (1 - sigmoid(x)) \n",
        "        elif(name == 'relu'):\n",
        "            return lambda x: np.heaviside(x, 0)\n",
        "\n",
        "if __name__=='__main__':\n",
        "    nn = NeuralNetwork([1, 100, 1], activations = ['sigmoid', 'sigmoid'])\n",
        "    X = 2 * np.pi * np.random.rand(15000).reshape(1, -1)\n",
        "    y = np.sin(X)\n",
        "    \n",
        "    nn.train(X, y, epochs = 5000, batch_size = 64, lr = .1)\n",
        "    _, a_s = nn.feedforward(X)\n",
        "    plt.scatter(X.flatten(), y.flatten())\n",
        "    plt.scatter(X.flatten(), a_s[-1].flatten())\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}